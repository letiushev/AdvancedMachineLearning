{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xPfgTCxpev8"
      },
      "source": [
        "# Assignment 3a - Image Captioning\n",
        "\n",
        "\n",
        "## Task description\n",
        "\n",
        "Given a subset of the COCO Image Captioning dataset, your task is to define hyperparameters to train an Encoder-Decoder with Attention model such that it achieves at least a BLEU score of 0.2 on the test set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR0con3OuFBM"
      },
      "source": [
        "## 0. Libraries / packages\n",
        "Necessary packages / library to import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyGWby4VpNMC"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from cv2 import imread, resize\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "import argparse\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S9rDk0Eg6FX"
      },
      "source": [
        "## 1. Utils\n",
        "Necessary helper functions to save your time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-eiVe6Qg4Op"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def init_embedding(embeddings):\n",
        "    \"\"\"\n",
        "    Fills embedding tensor with values from the uniform distribution.\n",
        "    :param embeddings: embedding tensor\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / embeddings.size(1))\n",
        "    torch.nn.init.uniform_(embeddings, -bias, bias)\n",
        "\n",
        "\n",
        "def load_embeddings(emb_file, word_map):\n",
        "    \"\"\"\n",
        "    Creates an embedding tensor for the specified word map, for loading into the model.\n",
        "    :param emb_file: file containing embeddings (stored in GloVe format)\n",
        "    :param word_map: word map\n",
        "    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Find embedding dimension\n",
        "    with open(emb_file, 'r') as f:\n",
        "        emb_dim = len(f.readline().split(' ')) - 1\n",
        "\n",
        "    vocab = set(word_map.keys())\n",
        "\n",
        "    # Create tensor to hold embeddings, initialize\n",
        "    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n",
        "    init_embedding(embeddings)\n",
        "\n",
        "    # Read embedding file\n",
        "    print(\"\\nLoading embeddings...\")\n",
        "    for line in open(emb_file, 'r'):\n",
        "        line = line.split(' ')\n",
        "\n",
        "        emb_word = line[0]\n",
        "        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n",
        "\n",
        "        # Ignore word if not in train_vocab\n",
        "        if emb_word not in vocab:\n",
        "            continue\n",
        "\n",
        "        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n",
        "\n",
        "    return embeddings, emb_dim\n",
        "\n",
        "\n",
        "def clip_gradient(optimizer, grad_clip):\n",
        "    \"\"\"\n",
        "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
        "    :param optimizer: optimizer with the gradients to be clipped\n",
        "    :param grad_clip: clip value\n",
        "    \"\"\"\n",
        "    for group in optimizer.param_groups:\n",
        "        for param in group['params']:\n",
        "            if param.grad is not None:\n",
        "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
        "\n",
        "\n",
        "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "                    bleu4, is_best):\n",
        "    \"\"\"\n",
        "    Saves model checkpoint.\n",
        "    :param data_name: base name of processed dataset\n",
        "    :param epoch: epoch number\n",
        "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param bleu4: validation BLEU-4 score for this epoch\n",
        "    :param is_best: is this checkpoint the best so far?\n",
        "    \"\"\"\n",
        "    state = {'epoch': epoch,\n",
        "             'epochs_since_improvement': epochs_since_improvement,\n",
        "             'bleu-4': bleu4,\n",
        "             'encoder': encoder,\n",
        "             'decoder': decoder,\n",
        "             'encoder_optimizer': encoder_optimizer,\n",
        "             'decoder_optimizer': decoder_optimizer}\n",
        "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
        "    if is_best:\n",
        "        torch.save(state, 'BEST_' + filename)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Keeps track of most recent, average, sum, and count of a metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def adjust_learning_rate(optimizer, shrink_factor):\n",
        "    \"\"\"\n",
        "    Shrinks learning rate by a specified factor.\n",
        "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
        "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nDECAYING learning rate.\")\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
        "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
        "\n",
        "\n",
        "def accuracy(scores, targets, k):\n",
        "    \"\"\"\n",
        "    Computes top-k accuracy, from predicted and true labels.\n",
        "    :param scores: scores from the model\n",
        "    :param targets: true labels\n",
        "    :param k: k in top-k accuracy\n",
        "    :return: top-k accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size = targets.size(0)\n",
        "    _, ind = scores.topk(k, 1, True, True)\n",
        "    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n",
        "    correct_total = correct.view(-1).float().sum()  # 0D tensor\n",
        "    return correct_total.item() * (100.0 / batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1tucmxFuklk"
      },
      "source": [
        "## 2. Dataset\n",
        "The dataset has already been pre-processed and split for you. There are 5000 images and captions for training, 1000 for validation and 1000 for testing. The images, captions, lenght of captions and vocabulary have been saved.\n",
        "\n",
        "The images have a resolution of (3, 256, 256) (channels, width, height). The captions have a maximum length of 100. Each image has 5 captions and the minimum word frequency is 5. \n",
        "\n",
        "The functions below facilitate the loading of the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFRBWOttkO-L"
      },
      "source": [
        "2.1) Data class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ng7Hn_gul3J"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class CaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, data_name, split, transform=None):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param data_name: base name of processed datasets\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        :param transform: image transform pipeline\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']\n",
        "\n",
        "        # Load encoded captions (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
        "            self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
        "            self.caplens = json.load(j)\n",
        "\n",
        "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'TRAIN':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oia74-j3kVJ7"
      },
      "source": [
        "2.2) Specify the location of your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By5NmLTP1SXa",
        "outputId": "cacf6f8d-514d-4eff-a7e6-41b769a241dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IIUfTVK4QqPlmhSwujNS_wBA1gA5g-Xa\n",
            "To: /content/coco_image_captions_preprocessed.zip\n",
            "100% 1.14G/1.14G [00:07<00:00, 156MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Download the dataset from my shared link https://drive.google.com/file/d/1h_oBXx8wNUXL-gKfuv68EEwrL3yIE9ae/view?usp=sharing\n",
        "!gdown --id 1IIUfTVK4QqPlmhSwujNS_wBA1gA5g-Xa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daeC-rSW33wS",
        "outputId": "19be2fe0-a215-4f02-c3a1-78dd74b4a2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  coco_image_captions_preprocessed.zip\n",
            "  inflating: coco_image_captions_preprocessed/TRAIN_CAPLENS_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/TRAIN_CAPTIONS_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/TRAIN_IMAGES_coco_5_cap_per_img_5_min_word_freq.hdf5  \n",
            "  inflating: coco_image_captions_preprocessed/VAL_CAPLENS_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/VAL_CAPTIONS_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/VAL_IMAGES_coco_5_cap_per_img_5_min_word_freq.hdf5  \n",
            "  inflating: coco_image_captions_preprocessed/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/TEST_CAPLENS_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/TEST_CAPTIONS_coco_5_cap_per_img_5_min_word_freq.json  \n",
            "  inflating: coco_image_captions_preprocessed/TEST_IMAGES_coco_5_cap_per_img_5_min_word_freq.hdf5  \n"
          ]
        }
      ],
      "source": [
        "!mkdir coco_image_captions_preprocessed/\n",
        "!unzip coco_image_captions_preprocessed.zip -d coco_image_captions_preprocessed/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V96lFRowkzEz"
      },
      "outputs": [],
      "source": [
        "# Data parameters\n",
        "data_folder = 'coco_image_captions_preprocessed/'  # folder with data files \n",
        "data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG9jrKpRw-xN"
      },
      "source": [
        "## 3. Model\n",
        "The Encoder-Decoder with Attention network is used for this task. The model is already defined, you have to set the hyperparameters. This is critical for achieving good results. \n",
        "\n",
        "**Please edit the cell 3.3)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ztXO8-lh-f",
        "outputId": "2b12c392-fcbb-4ce8-e76e-315ebd1155a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #make sure that you are using GPU acceleration\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQDmlw78lqEE"
      },
      "source": [
        "#### 3.1) Encoder\n",
        "The class Encoder defines the encoder of our model. It is initialized with ResNet101 pretrained weights, so it is capable of extracting relevant features and the encoded image size is 14. You can change the network backbone and the encoded size to make it faster or more accurate, but I recommend you to leave it as it is and only change the hyperparameters mentioned in the task description.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2AxpZ2-loc_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztDGl8r0mf2P"
      },
      "source": [
        "#### 3.2) Decoder with Attention\n",
        "The class Attention defines the attention network for our decoder. The class Decoder defines the decoder of our model which takes the encoded image and generates captions. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFxXPq1anNqa"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n",
        "\n",
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q30U793o1Vr"
      },
      "source": [
        "#### **3.3) Set hyperparameters for the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l3Gh87wo-Sn"
      },
      "outputs": [],
      "source": [
        "# Model parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5 #dropout for regularization\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIJBX1mUaMpl"
      },
      "source": [
        "A Word Embedding is just a mapping from words to vectors. Dimensionality in word embeddings refers to the length of these vectors. 128 256 512\n",
        "\n",
        "Attention layer- while encoding or “reading” the image, only one part of the image gets focused on at each time step. And similarly, while writing, only a certain part of the image gets generated at that time-step.Also 2 to the power of n (128 256 512)\n",
        "\n",
        "The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence. So they should have similar dimension\n",
        "\n",
        "\n",
        "Dropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel.A good value for dropout in a hidden layer is between 0.5 and 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgNpxIykpt5V"
      },
      "source": [
        "## 4. Training\n",
        "Here you have to train the model. Please specify the word_map_file location (the vocabulary which is together with the given dataset). \n",
        "The training and validation script are already defined.\n",
        "\n",
        "**Setup the hyperparameters for the optimization on the cell 4.1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq-sLKFUqm-_"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F--JJpjEq_dv"
      },
      "source": [
        "#### **4.1) Setup optimization hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmXYs1G3rECt"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "start_epoch = 0\n",
        "epochs = 11 # number of epochs to train for (if early stopping is not triggered)\n",
        "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32 \n",
        "workers = 0  # for data-loading; avoid changing this, there might be some errors\n",
        "encoder_lr = 0.001 # learning rate for encoder if fine-tuning\n",
        "decoder_lr =  0.001 # learning rate for decoder\n",
        "grad_clip = 5.  # clip gradients at an absolute value of\n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
        "best_bleu4 = 0.  # BLEU-4 score right now\n",
        "print_freq = 100  # print training/validation stats every __ batches\n",
        "fine_tune_encoder = False  # True if you want to fine-tune encoder and False otherwise. Choose wisely.\n",
        "checkpoint = None # path to checkpoint, None if none"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAvO0wx6pVoI"
      },
      "source": [
        "Specifically, the learning rate is a configurable hyperparameter used in the training of neural networks that has a small positive value, often in the range between 0.0 and 1.0. The learning rate controls how quickly the model is adapted to the problem.\n",
        "\n",
        "The objective of fine-tuning is to adjust the weights of the trained model from the previous phase to improve the prediction outcome by using all the training dataset. This procedure utilizes a loss function and gradient descent algorithm from the stacked sparse auto-encoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrJ3mVsEeHir"
      },
      "source": [
        "A traditional default value for the learning rate is 0.1 or 0.001\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6i4uQvu0m5y"
      },
      "source": [
        "#### 4.2) Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoABDZtIpxcZ"
      },
      "outputs": [],
      "source": [
        "# Read word map\n",
        "word_map_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "    \n",
        "\n",
        "    \n",
        "# Custom dataloaders\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
        "    batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucRB1vbYqszj"
      },
      "source": [
        "#### 4.3) You can change the checkpoint parameter on the code cell 4.1 in case you interrupted your training and would like to train from a previous checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68,
          "referenced_widgets": [
            "7ec369ea10fe42e89e4451bb84000d80",
            "7b3a932289cd43258a1e9c27a773d1ac",
            "b3b0c09d66584a7493e6c8791247f689",
            "29435e4dc7ce43a383b5ebc072f1e625",
            "55391d9d299247c799b296c52e42d57e",
            "dd4f3f2265a641bda61321fe4168f560",
            "40a3dbdc0fc44e439b6f7600bb2e84b1",
            "8ad6419e9e8941e3915c61090fb24d1a",
            "259248a34aa34dd1b8232590a9975217",
            "84db91911128414da1741fd38dce2358",
            "dc44a5b4036e4b7e8a7076bc9c1cb0c7"
          ]
        },
        "id": "28rAG8hxqrfR",
        "outputId": "47ac0e08-7406-4eb6-b7b7-ad413fbc924d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ec369ea10fe42e89e4451bb84000d80",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize / load checkpoint\n",
        "if checkpoint is None:\n",
        "    decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                   embed_dim=emb_dim,\n",
        "                                   decoder_dim=decoder_dim,\n",
        "                                   vocab_size=len(word_map),\n",
        "                                   dropout=dropout)\n",
        "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                         lr=decoder_lr)\n",
        "    encoder = Encoder()\n",
        "    encoder.fine_tune(fine_tune_encoder)\n",
        "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                         lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "else:\n",
        "    checkpoint = torch.load(checkpoint)\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "    best_bleu4 = checkpoint['bleu-4']\n",
        "    decoder = checkpoint['decoder']\n",
        "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
        "    encoder = checkpoint['encoder']\n",
        "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
        "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr)\n",
        "# Move to GPU, if available\n",
        "decoder = decoder.to(device)\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAmRIeh5rID4"
      },
      "source": [
        "#### 4.4) Training and validation script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1MY-AHprKCw"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to GPU, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        imgs = encoder(imgs)\n",
        "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = caps_sorted[:, 1:]\n",
        "\n",
        "        # Remove timesteps that we didn't decode at, or are pads\n",
        "        # pack_padded_sequence is an easy trick to do this\n",
        "        scores, *_ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "        targets, *_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop.\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          batch_time=batch_time,\n",
        "                                                                          data_time=data_time, loss=losses,\n",
        "                                                                          top5=top5accs))\n",
        "\n",
        "\n",
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
        "    # solves the issue #57\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "\n",
        "            # Move to device, if available\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            if encoder is not None:\n",
        "                imgs = encoder(imgs)\n",
        "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "            targets = caps_sorted[:, 1:]\n",
        "\n",
        "            # Remove timesteps that we didn't decode at, or are pads\n",
        "            # pack_padded_sequence is an easy trick to do this\n",
        "            scores_copy = scores.clone()\n",
        "            scores, *_ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
        "            targets, *_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z5I7VeQxQXd"
      },
      "source": [
        "Train your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9HF9m0ixP1V",
        "outputId": "a2f3e468-d498-4f91-d4e1-58b1af3ee432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: [0][0/782]\tBatch Time 3.551 (3.551)\tData Load Time 0.151 (0.151)\tLoss 10.0823 (10.0823)\tTop-5 Accuracy 0.000 (0.000)\n",
            "Epoch: [0][100/782]\tBatch Time 0.907 (0.943)\tData Load Time 0.070 (0.072)\tLoss 6.1480 (6.6546)\tTop-5 Accuracy 37.766 (34.054)\n",
            "Epoch: [0][200/782]\tBatch Time 0.900 (0.929)\tData Load Time 0.071 (0.072)\tLoss 5.2815 (6.1932)\tTop-5 Accuracy 50.275 (38.177)\n",
            "Epoch: [0][300/782]\tBatch Time 0.898 (0.921)\tData Load Time 0.072 (0.071)\tLoss 5.2059 (5.9212)\tTop-5 Accuracy 50.997 (41.192)\n",
            "Epoch: [0][400/782]\tBatch Time 0.927 (0.919)\tData Load Time 0.077 (0.071)\tLoss 5.1020 (5.7188)\tTop-5 Accuracy 49.867 (43.592)\n",
            "Epoch: [0][500/782]\tBatch Time 0.900 (0.918)\tData Load Time 0.072 (0.071)\tLoss 4.6833 (5.5699)\tTop-5 Accuracy 55.679 (45.509)\n",
            "Epoch: [0][600/782]\tBatch Time 0.951 (0.917)\tData Load Time 0.071 (0.071)\tLoss 4.8063 (5.4511)\tTop-5 Accuracy 53.608 (46.973)\n",
            "Epoch: [0][700/782]\tBatch Time 0.908 (0.917)\tData Load Time 0.076 (0.071)\tLoss 4.7128 (5.3552)\tTop-5 Accuracy 56.069 (48.208)\n",
            "Validation: [0/157]\tBatch Time 0.693 (0.693)\tLoss 3.7896 (3.7896)\tTop-5 Accuracy 67.575 (67.575)\t\n",
            "Validation: [100/157]\tBatch Time 0.715 (0.701)\tLoss 4.1696 (4.5244)\tTop-5 Accuracy 62.599 (58.615)\t\n",
            "\n",
            " * LOSS - 4.516, TOP-5 ACCURACY - 58.662, BLEU-4 - 0.10246258993851638\n",
            "\n",
            "Epoch: [1][0/782]\tBatch Time 0.948 (0.948)\tData Load Time 0.077 (0.077)\tLoss 4.6330 (4.6330)\tTop-5 Accuracy 55.978 (55.978)\n",
            "Epoch: [1][100/782]\tBatch Time 0.893 (0.910)\tData Load Time 0.073 (0.071)\tLoss 4.0196 (4.5048)\tTop-5 Accuracy 61.773 (58.497)\n",
            "Epoch: [1][200/782]\tBatch Time 0.898 (0.909)\tData Load Time 0.071 (0.071)\tLoss 4.4991 (4.4938)\tTop-5 Accuracy 60.563 (58.537)\n",
            "Epoch: [1][300/782]\tBatch Time 0.915 (0.909)\tData Load Time 0.070 (0.071)\tLoss 3.9945 (4.4810)\tTop-5 Accuracy 66.120 (58.765)\n",
            "Epoch: [1][400/782]\tBatch Time 0.879 (0.911)\tData Load Time 0.068 (0.071)\tLoss 4.3504 (4.4649)\tTop-5 Accuracy 59.437 (59.010)\n",
            "Epoch: [1][500/782]\tBatch Time 0.919 (0.911)\tData Load Time 0.069 (0.071)\tLoss 4.2432 (4.4481)\tTop-5 Accuracy 63.333 (59.261)\n",
            "Epoch: [1][600/782]\tBatch Time 0.918 (0.911)\tData Load Time 0.071 (0.071)\tLoss 4.3678 (4.4292)\tTop-5 Accuracy 62.022 (59.577)\n",
            "Epoch: [1][700/782]\tBatch Time 0.861 (0.911)\tData Load Time 0.067 (0.071)\tLoss 4.1597 (4.4101)\tTop-5 Accuracy 63.112 (59.806)\n",
            "Validation: [0/157]\tBatch Time 0.693 (0.693)\tLoss 4.0079 (4.0079)\tTop-5 Accuracy 68.633 (68.633)\t\n",
            "Validation: [100/157]\tBatch Time 0.702 (0.701)\tLoss 4.3899 (4.2000)\tTop-5 Accuracy 62.133 (63.185)\t\n",
            "\n",
            " * LOSS - 4.206, TOP-5 ACCURACY - 63.109, BLEU-4 - 0.1292027045068202\n",
            "\n",
            "Epoch: [2][0/782]\tBatch Time 0.960 (0.960)\tData Load Time 0.071 (0.071)\tLoss 4.4025 (4.4025)\tTop-5 Accuracy 57.029 (57.029)\n",
            "Epoch: [2][100/782]\tBatch Time 0.884 (0.914)\tData Load Time 0.068 (0.071)\tLoss 3.8421 (4.1198)\tTop-5 Accuracy 66.575 (63.491)\n",
            "Epoch: [2][200/782]\tBatch Time 0.911 (0.914)\tData Load Time 0.071 (0.071)\tLoss 4.4538 (4.1288)\tTop-5 Accuracy 58.355 (63.369)\n",
            "Epoch: [2][300/782]\tBatch Time 0.875 (0.911)\tData Load Time 0.070 (0.070)\tLoss 4.0960 (4.1086)\tTop-5 Accuracy 65.812 (63.665)\n",
            "Epoch: [2][400/782]\tBatch Time 0.942 (0.911)\tData Load Time 0.070 (0.070)\tLoss 3.7299 (4.1050)\tTop-5 Accuracy 68.966 (63.659)\n",
            "Epoch: [2][500/782]\tBatch Time 0.919 (0.911)\tData Load Time 0.069 (0.071)\tLoss 4.2142 (4.1058)\tTop-5 Accuracy 59.897 (63.665)\n",
            "Epoch: [2][600/782]\tBatch Time 0.905 (0.911)\tData Load Time 0.069 (0.070)\tLoss 3.8268 (4.1029)\tTop-5 Accuracy 68.717 (63.687)\n",
            "Epoch: [2][700/782]\tBatch Time 0.934 (0.911)\tData Load Time 0.068 (0.071)\tLoss 4.2432 (4.0947)\tTop-5 Accuracy 60.215 (63.805)\n",
            "Validation: [0/157]\tBatch Time 0.707 (0.707)\tLoss 4.0713 (4.0713)\tTop-5 Accuracy 66.230 (66.230)\t\n",
            "Validation: [100/157]\tBatch Time 0.692 (0.700)\tLoss 3.9997 (4.0454)\tTop-5 Accuracy 66.213 (65.198)\t\n",
            "\n",
            " * LOSS - 4.054, TOP-5 ACCURACY - 65.016, BLEU-4 - 0.13630049141603431\n",
            "\n",
            "Epoch: [3][0/782]\tBatch Time 0.965 (0.965)\tData Load Time 0.074 (0.074)\tLoss 4.1420 (4.1420)\tTop-5 Accuracy 64.023 (64.023)\n",
            "Epoch: [3][100/782]\tBatch Time 0.886 (0.906)\tData Load Time 0.070 (0.071)\tLoss 4.1724 (3.8974)\tTop-5 Accuracy 60.167 (66.010)\n",
            "Epoch: [3][200/782]\tBatch Time 0.887 (0.907)\tData Load Time 0.068 (0.070)\tLoss 3.9246 (3.9012)\tTop-5 Accuracy 63.912 (66.040)\n",
            "Epoch: [3][300/782]\tBatch Time 0.873 (0.906)\tData Load Time 0.069 (0.070)\tLoss 3.9860 (3.8919)\tTop-5 Accuracy 64.368 (66.080)\n",
            "Epoch: [3][400/782]\tBatch Time 0.906 (0.907)\tData Load Time 0.072 (0.070)\tLoss 4.0492 (3.8835)\tTop-5 Accuracy 63.859 (66.232)\n",
            "Epoch: [3][500/782]\tBatch Time 0.881 (0.907)\tData Load Time 0.070 (0.070)\tLoss 3.7844 (3.8832)\tTop-5 Accuracy 67.787 (66.277)\n",
            "Epoch: [3][600/782]\tBatch Time 0.926 (0.907)\tData Load Time 0.075 (0.070)\tLoss 3.7421 (3.8881)\tTop-5 Accuracy 67.742 (66.234)\n",
            "Epoch: [3][700/782]\tBatch Time 0.879 (0.906)\tData Load Time 0.070 (0.070)\tLoss 3.8733 (3.8849)\tTop-5 Accuracy 65.439 (66.299)\n",
            "Validation: [0/157]\tBatch Time 0.679 (0.679)\tLoss 4.0959 (4.0959)\tTop-5 Accuracy 64.067 (64.067)\t\n",
            "Validation: [100/157]\tBatch Time 0.694 (0.698)\tLoss 4.1370 (3.9787)\tTop-5 Accuracy 63.115 (65.814)\t\n",
            "\n",
            " * LOSS - 3.965, TOP-5 ACCURACY - 66.136, BLEU-4 - 0.1467187746333137\n",
            "\n",
            "Epoch: [4][0/782]\tBatch Time 0.963 (0.963)\tData Load Time 0.090 (0.090)\tLoss 3.9753 (3.9753)\tTop-5 Accuracy 64.908 (64.908)\n",
            "Epoch: [4][100/782]\tBatch Time 0.914 (0.903)\tData Load Time 0.072 (0.070)\tLoss 3.5779 (3.7247)\tTop-5 Accuracy 72.118 (68.311)\n",
            "Epoch: [4][200/782]\tBatch Time 0.913 (0.904)\tData Load Time 0.069 (0.070)\tLoss 4.1644 (3.7092)\tTop-5 Accuracy 61.376 (68.417)\n",
            "Epoch: [4][300/782]\tBatch Time 0.892 (0.905)\tData Load Time 0.069 (0.070)\tLoss 3.3517 (3.7252)\tTop-5 Accuracy 73.333 (68.193)\n",
            "Epoch: [4][400/782]\tBatch Time 0.891 (0.905)\tData Load Time 0.072 (0.070)\tLoss 3.6881 (3.7263)\tTop-5 Accuracy 68.056 (68.178)\n",
            "Epoch: [4][500/782]\tBatch Time 0.888 (0.905)\tData Load Time 0.072 (0.070)\tLoss 3.6809 (3.7239)\tTop-5 Accuracy 67.688 (68.213)\n",
            "Epoch: [4][600/782]\tBatch Time 0.885 (0.905)\tData Load Time 0.069 (0.070)\tLoss 3.5986 (3.7209)\tTop-5 Accuracy 69.296 (68.252)\n",
            "Epoch: [4][700/782]\tBatch Time 0.934 (0.906)\tData Load Time 0.072 (0.070)\tLoss 3.9122 (3.7166)\tTop-5 Accuracy 66.667 (68.324)\n",
            "Validation: [0/157]\tBatch Time 0.685 (0.685)\tLoss 3.5793 (3.5793)\tTop-5 Accuracy 69.399 (69.399)\t\n",
            "Validation: [100/157]\tBatch Time 0.692 (0.696)\tLoss 3.8450 (3.8908)\tTop-5 Accuracy 65.952 (67.124)\t\n",
            "\n",
            " * LOSS - 3.907, TOP-5 ACCURACY - 66.891, BLEU-4 - 0.15192211606317274\n",
            "\n",
            "Epoch: [5][0/782]\tBatch Time 0.929 (0.929)\tData Load Time 0.081 (0.081)\tLoss 3.6731 (3.6731)\tTop-5 Accuracy 70.977 (70.977)\n",
            "Epoch: [5][100/782]\tBatch Time 0.954 (0.902)\tData Load Time 0.071 (0.071)\tLoss 4.0259 (3.5719)\tTop-5 Accuracy 64.171 (69.970)\n",
            "Epoch: [5][200/782]\tBatch Time 0.894 (0.908)\tData Load Time 0.073 (0.070)\tLoss 3.4720 (3.5719)\tTop-5 Accuracy 70.112 (70.027)\n",
            "Epoch: [5][300/782]\tBatch Time 0.880 (0.907)\tData Load Time 0.069 (0.070)\tLoss 3.1827 (3.5687)\tTop-5 Accuracy 78.510 (70.180)\n",
            "Epoch: [5][400/782]\tBatch Time 0.871 (0.907)\tData Load Time 0.067 (0.070)\tLoss 3.4021 (3.5727)\tTop-5 Accuracy 71.795 (70.120)\n",
            "Epoch: [5][500/782]\tBatch Time 0.897 (0.906)\tData Load Time 0.072 (0.070)\tLoss 3.2282 (3.5714)\tTop-5 Accuracy 76.836 (70.156)\n",
            "Epoch: [5][600/782]\tBatch Time 0.901 (0.906)\tData Load Time 0.070 (0.070)\tLoss 3.4411 (3.5737)\tTop-5 Accuracy 73.297 (70.132)\n",
            "Epoch: [5][700/782]\tBatch Time 0.915 (0.906)\tData Load Time 0.066 (0.070)\tLoss 3.3787 (3.5726)\tTop-5 Accuracy 72.500 (70.163)\n",
            "Validation: [0/157]\tBatch Time 0.686 (0.686)\tLoss 4.0819 (4.0819)\tTop-5 Accuracy 64.365 (64.365)\t\n",
            "Validation: [100/157]\tBatch Time 0.689 (0.695)\tLoss 3.8717 (3.8733)\tTop-5 Accuracy 68.245 (67.615)\t\n",
            "\n",
            " * LOSS - 3.870, TOP-5 ACCURACY - 67.644, BLEU-4 - 0.15430579552567972\n",
            "\n",
            "Epoch: [6][0/782]\tBatch Time 0.948 (0.948)\tData Load Time 0.091 (0.091)\tLoss 3.4989 (3.4989)\tTop-5 Accuracy 72.951 (72.951)\n",
            "Epoch: [6][100/782]\tBatch Time 0.919 (0.906)\tData Load Time 0.065 (0.069)\tLoss 3.4382 (3.4446)\tTop-5 Accuracy 73.514 (71.915)\n",
            "Epoch: [6][200/782]\tBatch Time 0.911 (0.907)\tData Load Time 0.070 (0.069)\tLoss 3.5286 (3.4474)\tTop-5 Accuracy 69.811 (71.851)\n",
            "Epoch: [6][300/782]\tBatch Time 0.898 (0.907)\tData Load Time 0.069 (0.069)\tLoss 3.1711 (3.4514)\tTop-5 Accuracy 76.011 (71.714)\n",
            "Epoch: [6][400/782]\tBatch Time 0.872 (0.907)\tData Load Time 0.069 (0.069)\tLoss 3.8286 (3.4532)\tTop-5 Accuracy 65.988 (71.703)\n",
            "Epoch: [6][500/782]\tBatch Time 0.868 (0.906)\tData Load Time 0.067 (0.069)\tLoss 3.3242 (3.4534)\tTop-5 Accuracy 75.706 (71.682)\n",
            "Epoch: [6][600/782]\tBatch Time 0.901 (0.906)\tData Load Time 0.069 (0.069)\tLoss 3.1910 (3.4541)\tTop-5 Accuracy 75.668 (71.656)\n",
            "Epoch: [6][700/782]\tBatch Time 0.894 (0.906)\tData Load Time 0.067 (0.069)\tLoss 3.4576 (3.4550)\tTop-5 Accuracy 70.370 (71.613)\n",
            "Validation: [0/157]\tBatch Time 0.679 (0.679)\tLoss 4.4494 (4.4494)\tTop-5 Accuracy 62.568 (62.568)\t\n",
            "Validation: [100/157]\tBatch Time 0.740 (0.694)\tLoss 3.9075 (3.8618)\tTop-5 Accuracy 67.595 (67.596)\t\n",
            "\n",
            " * LOSS - 3.847, TOP-5 ACCURACY - 67.833, BLEU-4 - 0.15891224154490685\n",
            "\n",
            "Epoch: [7][0/782]\tBatch Time 0.959 (0.959)\tData Load Time 0.076 (0.076)\tLoss 3.6835 (3.6835)\tTop-5 Accuracy 68.044 (68.044)\n",
            "Epoch: [7][100/782]\tBatch Time 0.915 (0.904)\tData Load Time 0.067 (0.068)\tLoss 3.2824 (3.3208)\tTop-5 Accuracy 73.641 (73.230)\n",
            "Epoch: [7][200/782]\tBatch Time 0.942 (0.906)\tData Load Time 0.068 (0.068)\tLoss 3.3962 (3.3190)\tTop-5 Accuracy 74.160 (73.260)\n",
            "Epoch: [7][300/782]\tBatch Time 0.890 (0.906)\tData Load Time 0.070 (0.068)\tLoss 3.2729 (3.3337)\tTop-5 Accuracy 73.816 (73.052)\n",
            "Epoch: [7][400/782]\tBatch Time 0.936 (0.906)\tData Load Time 0.065 (0.068)\tLoss 3.5761 (3.3377)\tTop-5 Accuracy 67.895 (73.042)\n",
            "Epoch: [7][500/782]\tBatch Time 0.877 (0.906)\tData Load Time 0.070 (0.068)\tLoss 3.1257 (3.3388)\tTop-5 Accuracy 76.011 (73.046)\n",
            "Epoch: [7][600/782]\tBatch Time 0.930 (0.906)\tData Load Time 0.070 (0.068)\tLoss 3.4697 (3.3476)\tTop-5 Accuracy 71.316 (72.939)\n",
            "Epoch: [7][700/782]\tBatch Time 0.876 (0.905)\tData Load Time 0.067 (0.068)\tLoss 3.3466 (3.3483)\tTop-5 Accuracy 73.130 (72.960)\n",
            "Validation: [0/157]\tBatch Time 0.690 (0.690)\tLoss 3.8477 (3.8477)\tTop-5 Accuracy 66.940 (66.940)\t\n",
            "Validation: [100/157]\tBatch Time 0.692 (0.696)\tLoss 4.0326 (3.8406)\tTop-5 Accuracy 65.668 (68.265)\t\n",
            "\n",
            " * LOSS - 3.840, TOP-5 ACCURACY - 68.227, BLEU-4 - 0.16170925899758773\n",
            "\n",
            "Epoch: [8][0/782]\tBatch Time 0.928 (0.928)\tData Load Time 0.081 (0.081)\tLoss 3.1516 (3.1516)\tTop-5 Accuracy 74.474 (74.474)\n",
            "Epoch: [8][100/782]\tBatch Time 0.897 (0.907)\tData Load Time 0.069 (0.068)\tLoss 3.2892 (3.2269)\tTop-5 Accuracy 72.145 (74.399)\n",
            "Epoch: [8][200/782]\tBatch Time 0.892 (0.907)\tData Load Time 0.068 (0.068)\tLoss 3.2650 (3.2200)\tTop-5 Accuracy 74.865 (74.652)\n",
            "Epoch: [8][300/782]\tBatch Time 0.909 (0.905)\tData Load Time 0.062 (0.067)\tLoss 3.2784 (3.2275)\tTop-5 Accuracy 72.386 (74.515)\n",
            "Epoch: [8][400/782]\tBatch Time 0.900 (0.904)\tData Load Time 0.066 (0.067)\tLoss 3.3548 (3.2332)\tTop-5 Accuracy 70.652 (74.464)\n",
            "Epoch: [8][500/782]\tBatch Time 0.898 (0.904)\tData Load Time 0.067 (0.068)\tLoss 3.2177 (3.2396)\tTop-5 Accuracy 76.022 (74.395)\n",
            "Epoch: [8][600/782]\tBatch Time 0.882 (0.904)\tData Load Time 0.069 (0.068)\tLoss 3.0901 (3.2404)\tTop-5 Accuracy 77.871 (74.375)\n",
            "Epoch: [8][700/782]\tBatch Time 0.872 (0.903)\tData Load Time 0.066 (0.068)\tLoss 3.1645 (3.2446)\tTop-5 Accuracy 78.286 (74.321)\n",
            "Validation: [0/157]\tBatch Time 0.696 (0.696)\tLoss 3.6586 (3.6586)\tTop-5 Accuracy 71.809 (71.809)\t\n",
            "Validation: [100/157]\tBatch Time 0.725 (0.694)\tLoss 4.3451 (3.8437)\tTop-5 Accuracy 63.570 (68.526)\t\n",
            "\n",
            " * LOSS - 3.838, TOP-5 ACCURACY - 68.406, BLEU-4 - 0.16260376441880717\n",
            "\n",
            "Epoch: [9][0/782]\tBatch Time 0.978 (0.978)\tData Load Time 0.075 (0.075)\tLoss 3.2814 (3.2814)\tTop-5 Accuracy 73.545 (73.545)\n",
            "Epoch: [9][100/782]\tBatch Time 0.913 (0.909)\tData Load Time 0.070 (0.069)\tLoss 2.9692 (3.1349)\tTop-5 Accuracy 79.508 (75.778)\n",
            "Epoch: [9][200/782]\tBatch Time 0.864 (0.906)\tData Load Time 0.064 (0.068)\tLoss 2.8351 (3.1342)\tTop-5 Accuracy 80.857 (75.813)\n",
            "Epoch: [9][300/782]\tBatch Time 0.863 (0.906)\tData Load Time 0.066 (0.068)\tLoss 3.0899 (3.1501)\tTop-5 Accuracy 76.833 (75.614)\n",
            "Epoch: [9][400/782]\tBatch Time 0.897 (0.904)\tData Load Time 0.067 (0.067)\tLoss 3.0721 (3.1485)\tTop-5 Accuracy 76.033 (75.659)\n",
            "Epoch: [9][500/782]\tBatch Time 0.899 (0.905)\tData Load Time 0.067 (0.067)\tLoss 3.1100 (3.1533)\tTop-5 Accuracy 76.503 (75.566)\n",
            "Epoch: [9][600/782]\tBatch Time 0.874 (0.905)\tData Load Time 0.069 (0.068)\tLoss 2.9586 (3.1537)\tTop-5 Accuracy 81.020 (75.577)\n",
            "Epoch: [9][700/782]\tBatch Time 0.893 (0.904)\tData Load Time 0.064 (0.068)\tLoss 3.3329 (3.1566)\tTop-5 Accuracy 72.800 (75.538)\n",
            "Validation: [0/157]\tBatch Time 0.677 (0.677)\tLoss 4.1967 (4.1967)\tTop-5 Accuracy 62.953 (62.953)\t\n",
            "Validation: [100/157]\tBatch Time 0.688 (0.693)\tLoss 3.8271 (3.8589)\tTop-5 Accuracy 67.857 (68.073)\t\n",
            "\n",
            " * LOSS - 3.838, TOP-5 ACCURACY - 68.406, BLEU-4 - 0.16361445623993973\n",
            "\n",
            "Epoch: [10][0/782]\tBatch Time 0.931 (0.931)\tData Load Time 0.070 (0.070)\tLoss 3.1700 (3.1700)\tTop-5 Accuracy 76.316 (76.316)\n",
            "Epoch: [10][100/782]\tBatch Time 0.881 (0.906)\tData Load Time 0.064 (0.068)\tLoss 2.7215 (3.0351)\tTop-5 Accuracy 80.055 (77.359)\n",
            "Epoch: [10][200/782]\tBatch Time 0.920 (0.904)\tData Load Time 0.063 (0.068)\tLoss 2.9956 (3.0312)\tTop-5 Accuracy 77.005 (77.311)\n",
            "Epoch: [10][300/782]\tBatch Time 0.892 (0.906)\tData Load Time 0.064 (0.068)\tLoss 3.0749 (3.0352)\tTop-5 Accuracy 77.534 (77.266)\n",
            "Epoch: [10][400/782]\tBatch Time 0.905 (0.905)\tData Load Time 0.069 (0.068)\tLoss 2.9125 (3.0403)\tTop-5 Accuracy 77.838 (77.187)\n",
            "Epoch: [10][500/782]\tBatch Time 0.908 (0.904)\tData Load Time 0.071 (0.068)\tLoss 2.8819 (3.0521)\tTop-5 Accuracy 77.926 (76.965)\n",
            "Epoch: [10][600/782]\tBatch Time 0.933 (0.905)\tData Load Time 0.066 (0.068)\tLoss 3.0942 (3.0607)\tTop-5 Accuracy 77.720 (76.867)\n",
            "Epoch: [10][700/782]\tBatch Time 0.904 (0.904)\tData Load Time 0.071 (0.068)\tLoss 3.1905 (3.0629)\tTop-5 Accuracy 74.667 (76.862)\n",
            "Validation: [0/157]\tBatch Time 0.688 (0.688)\tLoss 3.8523 (3.8523)\tTop-5 Accuracy 69.452 (69.452)\t\n",
            "Validation: [100/157]\tBatch Time 0.688 (0.695)\tLoss 3.4892 (3.8551)\tTop-5 Accuracy 73.034 (68.166)\t\n",
            "\n",
            " * LOSS - 3.849, TOP-5 ACCURACY - 68.373, BLEU-4 - 0.16508833116474583\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Epochs\n",
        "for epoch in range(start_epoch, epochs):\n",
        "\n",
        "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "    if epochs_since_improvement == 20:\n",
        "        break\n",
        "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "        if fine_tune_encoder:\n",
        "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
        "\n",
        "    # One epoch's training\n",
        "    train(train_loader=train_loader,\n",
        "          encoder=encoder,\n",
        "          decoder=decoder,\n",
        "          criterion=criterion,\n",
        "          encoder_optimizer=encoder_optimizer,\n",
        "          decoder_optimizer=decoder_optimizer,\n",
        "          epoch=epoch)\n",
        "\n",
        "    # One epoch's validation\n",
        "    recent_bleu4 = validate(val_loader=val_loader,\n",
        "                            encoder=encoder,\n",
        "                            decoder=decoder,\n",
        "                            criterion=criterion)\n",
        "\n",
        "    # Check if there was an improvement\n",
        "    is_best = recent_bleu4 > best_bleu4\n",
        "    best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "    if not is_best:\n",
        "        epochs_since_improvement += 1\n",
        "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "    else:\n",
        "        epochs_since_improvement = 0\n",
        "\n",
        "    # Save checkpoint\n",
        "    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
        "                    decoder_optimizer, recent_bleu4, is_best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ReFruQazkvQ"
      },
      "source": [
        "## 5. Evaluation on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZQLZ1x70CGm"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "def evaluate(beam_size):\n",
        "    \"\"\"\n",
        "    Evaluation\n",
        "    :param beam_size: beam size at which to generate captions for evaluation\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # DataLoader\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n",
        "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
        "\n",
        "    # TODO: Batched Beam Search\n",
        "    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n",
        "\n",
        "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
        "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
        "\n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = top_k_words / vocab_size  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds.long()], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != word_map['<end>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds].long()]\n",
        "            c = c[prev_word_inds[incomplete_inds].long()]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds].long()]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "    bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dlYatuK1BJi"
      },
      "source": [
        "Evaluate your model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZVFTdaD0K_6"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "data_folder = 'coco_image_captions_preprocessed/'  # folder with data files \n",
        "data_name = 'coco_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
        "checkpoint = 'BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n",
        "word_map_file = 'coco_image_captions_preprocessed/WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(checkpoint)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "# Load word map (word2ix)\n",
        "with open(word_map_file, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}\n",
        "vocab_size = len(word_map)\n",
        "\n",
        "# Normalization transform\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSutrHDs0NiL",
        "outputId": "414f653d-77be-42c4-83bc-ec6eff435ddd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "EVALUATING AT BEAM SIZE 1: 100%|██████████| 5000/5000 [05:15<00:00, 15.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "BLEU-4 score @ beam size of 1 is 0.2220.\n"
          ]
        }
      ],
      "source": [
        "beam_size = 1\n",
        "print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "hw3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "259248a34aa34dd1b8232590a9975217": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29435e4dc7ce43a383b5ebc072f1e625": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_259248a34aa34dd1b8232590a9975217",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ad6419e9e8941e3915c61090fb24d1a",
            "value": 178793939
          }
        },
        "40a3dbdc0fc44e439b6f7600bb2e84b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55391d9d299247c799b296c52e42d57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc44a5b4036e4b7e8a7076bc9c1cb0c7",
            "placeholder": "​",
            "style": "IPY_MODEL_84db91911128414da1741fd38dce2358",
            "value": " 171M/171M [00:01&lt;00:00, 88.6MB/s]"
          }
        },
        "7b3a932289cd43258a1e9c27a773d1ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec369ea10fe42e89e4451bb84000d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3b0c09d66584a7493e6c8791247f689",
              "IPY_MODEL_29435e4dc7ce43a383b5ebc072f1e625",
              "IPY_MODEL_55391d9d299247c799b296c52e42d57e"
            ],
            "layout": "IPY_MODEL_7b3a932289cd43258a1e9c27a773d1ac"
          }
        },
        "84db91911128414da1741fd38dce2358": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ad6419e9e8941e3915c61090fb24d1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b3b0c09d66584a7493e6c8791247f689": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40a3dbdc0fc44e439b6f7600bb2e84b1",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4f3f2265a641bda61321fe4168f560",
            "value": "100%"
          }
        },
        "dc44a5b4036e4b7e8a7076bc9c1cb0c7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4f3f2265a641bda61321fe4168f560": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
